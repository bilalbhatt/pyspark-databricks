The map() transformation in PySpark is used to apply a function to each element in a dataset. 
This function takes a single element as input and returns a transformed element as output.
The map() transformation returns a new dataset that consists of the transformed elements.

data=[('bilal', 'bhat'),('mohd', 'Dilawar')]
rdd=spark.sparkContext.parallelize(data)
rdd1=rdd.map(lambda x: x +( x[0] + " " + x[1],))
print(rdd1.collect())  
